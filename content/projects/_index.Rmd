---
title: "Research"
author: Brad Carruthers
output: html_document
---

Here is some of the research I am currently working on.

### MSc Thesis at LSE: Nudging Online Users Into Asking Better Questions

So after bouncing around quite a bit, I've settled on a topic for my thesis at LSE. I thought it would be cool if there was a way to help users identify whether their questions online have a high or low chance of being answered in a constructive way. The various <a href="https://stackexchange.com/sites#questions" target="_blank">StackExchange</a> fora provide a neat set of large datasets to play with (see <a href="https://archive.org/download/stackexchange" target="_blank">here</a> for the data dump), and it doesn't appear to difficult to use data on historial questions and answers to predict some kind of "quality" inherent in questions - i.e. as in <a href="https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewFile/8080/8143" target="_blank">Ravi *et al.* (2014)</a>.

Ravi *et al.* (2014) basically use the text of StackOverflow questions to predict their score (upvotes minus downvotes) within the StackOverflow community. Their fancy, suped up model (which is classification regression on Latent Dirichlet Allocation-steroids) manages 72% classification accuracy, beating the 60% accuracy if only page-views (i.e. popularity) are used for prediction.

Since my aim is to inform users of whether their question will illicit a productive response from a community, I am considering other response variables like the number of edits a question undergoes and the sentiment of comments/answers...

---

### Honours Thesis at Stellenbosch: Teacher job satisfaction and learner performance: An investigation into the 2004 Grade 6 Intermediate Phase Systemic Evaluation

<a href="/img/honours-thesis.pdf" target="_blank">This</a> is my thesis for my Honours in Economics at Stellenbosch University, South Africa.